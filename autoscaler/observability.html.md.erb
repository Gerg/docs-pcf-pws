---
title: Using Debug Logging with App Autoscaler
owner: Autoscaler
---

This topic describes how to monitor App Autoscaler and its supporting VMware Tanzu Application Service for VMs (TAS for VMs) components through debug logging.


## <a id='overview'></a> Overview of Debug Logging in Autoscaler

[Insert summary of topic here.]


## <a id='allow-debug'></a> Configure Debug Logging for Autoscaler

[Does the **Do not forward debug logs** checkbox need to be configured at all before you can configure debug logging for Autoscaler?]

To configure debug logging for Autoscaler:

1. In a terminal window, run:

    ```
    cf target -o system -s autoscaling
    ```

1. Set the logging level for Autoscaler to `debug` by running:

    ```
    cf set-env autoscale LOG_LEVEL debug
    ```

1. Restart Autoscaler by running:

    ```
    cf restart autoscale
    ```
    <p class='note'><strong>Note:</strong> Re-starting Autoscaler briefly interrupts any processes it is currently performing.</p>

[What about procedures for viewing debug logs and autoscaling events?]

## <a id='observability'></a> Reviewing Autoscaler Usage

As an operator, you can use debug logs to see whether developers of your apps are using Autoscaler to scale their apps, as well as [what are the running app instance metrics and Cloud Controller API endpoint info below supposed to tell you?]

If you have configured debug logging for Autoscaler, Autoscaler logs the following metrics [you run `cf logs autoscale` to see these, right?]:

* `autoscale.scale_up`: the number of times Autoscaler has scaled an app up
* `autoscale.scale_down`: the number of times Autoscaler has scaled an app down
* `autoscale.enabled_bindings`: the number of Autoscaler service bindings in [the entire TAS for VMs deployment? A particular space and/or org?]

<p class='note'><strong>Note:</strong> These metrics appear as log lines and are not currently emitted to Loggregator.</p>

You can also review the overall number of running app instances that Diego reports [and find out what, exactly?]. For more information, see [Running App Instances, Rate of Change](../../operating/monitoring/kpi.html#1hraverageofbbs.LRPsRunning) in _Key Performance Indicators_.

Because each team of developers may use different autoscaling rules for their apps, it may be difficult to make generalizations [about what?]. However, you can also overlay [compare?] the number of running app instances with other scaling metrics that the developers of your apps may be using [to determine what?]. For more information about various scaling metrics, see [Key Performance Indicators](../../operating/monitoring/kpi.html).

You can also query the `/v3/service_instances` endpoint of the Cloud Foundry API (CAPI), filtering by service plans that are associated with Autoscaler [how do you do this?]. [What sort of information does this provide?] For more information, see the [CAPI documentation](https://v3-apidocs.cloudfoundry.org/version/3.117.0/index.html#list-service-instances). [The original PR says "Cloud Controller API", but I've been having trouble finding out whether there really is a separate Cloud Controller API or it's part of the Cloud Foundry API.]


## <a id='failure-modes'></a> Failure Modes

Instead of always maintaining enough app instances to handle the maximum amount of traffic that you expect for your apps, Autoscaler allows you to provision only the number of app instances that are necessary based on metrics and your defined autoscaling rules. However, if Autoscaler or the TAS for VMs components that support Autoscaler fail, Autoscaler cannot scale the number of app instances up enough to meet the demands of your app usage.

If you are not using Autoscaler, TAS for VMs component failures might only impact the control plane [what does "control plane" mean in this context?] of your TAS for VMs deployment. Using Autoscaler adds a harder dependency on these components being available. [Does "adding a harder dependency" mean it's more important that these components remain available, or that increased demand makes it less likely that they'll be available?]

This section describes various ways in which the TAS for VMs components that support Autoscaler can fail. This is a non-exhaustive list. [Are the ones listed here the most common ones?] Each section lists the following:

* Relevant Autoscaler debug log metrics
* Example messages that appear in the output of `cf autoscaling-events`
* Example errors in the Autoscaler debug log output

### <a id='log-cache'></a> Log Cache

Autoscaler depends heavily on Log Cache, which provides the majority of metrics that Autoscaler uses to determine when to scale an app. If Log Cache fails or
is insufficiently scaled, Autoscaler cannot retrieve metrics, so it cannot determine when to scale an app.

For more information about Log Cache, see [Logging and Metrics Architecture](../../loggregator/architecture.html) and [Log
Components](../../loggregator/agent-architecture.html#log-egress) in _Log and Metric Agent Architecture (Beta)_.

#### <a id='lc-debug-log-metrics'></a> Debug Log Metrics

Autoscaler logs the following metrics:

* `autoscale.logcache.read_errors`: errors that Autoscaler encounters when reading from Log Cache
* `autoscale.logcache_metric_fetch_duration`: the time Autoscaler takes to retrieve metrics from Log Cache

<p class='note'><strong>Note:</strong> These metrics appear as log lines and are not currently emitted to Loggregator.</p>

#### <a id='lc-gateway-unavailable'></a> Log Cache Gateway is Unavailable

When the Log Cache gateway is unavailable [as in it's not there/running at all, or Autoscaler simply fails to connect to it?], you see an autoscaling event similar to the following example:

<pre class='terminal'>
Autoscaler did not receive any metrics for memory during the scaling window. Scaling down will be deferred until these metrics are available.
</pre>

You also see app logs similar to the following example:

<pre class='terminal'>[APP/PROC/WEB/2] OUT logcache walker: unexpected status code 502</pre>
[This is only a segment of what the actual full debug log looks like, right? If so, what would it look like as a full debug log?]

#### <a id='lc-node-failure'></a> Individual Log Cache Node Fails

Log Cache holds the log and metric envelopes for an app on a single Log Cache node. ["Node" = just a VM? Or physical server, worker machine, and/or VM?] When that Log Cache node fails, Autoscaler cannot retrieve those envelopes, so it cannot determine when to scale an app.

When the Log Cache node that holds the envelopes for an app fails, you see an autoscaling event similar to the following example:

<pre class='terminal'>
Autoscaler did not receive any metrics for memory during the scaling window. Scaling down will be deferred until these metrics are available.
</pre>

You also see app logs similar to the following example:

<pre class='terminal'>[APP/PROC/WEB/1] OUT logcache walker: unexpected status code 503</pre>

#### <a id='lc-node-unresponsive'></a> Log Cache Nodes are Unresponsive

When Log Cache nodes do not respond to requests from Autoscaler, you see an autoscaling event similar to the following example:

<pre class='terminal'>
Autoscaler did not receive any metrics for memory during the scaling window. Scaling down will be deferred until these metrics are available.
</pre>

You also see app logs similar to the following example:

<pre class='terminal'>
[APP/PROC/WEB/2] OUT logcache walker: Get "https://log-cache.sys.example.com/api/v1/read/85a6aff3-c739-4364-8df3-44f1aa50662b": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
</pre>

### <a id='cloud-controller'></a> Cloud Controller

Autoscaler reads the app metadata that the Cloud Controller stores to determine how many instances an app currently has. When Autoscaler scales the number of
app instances up or down, it sends a request to the Cloud Controller. Then, the Cloud Controller directs the Diego Cell that runs the app to stage the number
of app instances that Autoscaler has requested.

Autoscaler also uses the Cloud Controller to retrieve information about service bindings for RabbitMQ queues. [Service bindings *to* RabbitMQ queues, or service binding information to put *into* RabbitMQ queues? Research how RabbitMQ queues work.]

The Cloud Controller also authorizes Log Cache to provide log and metric envelopes to Autoscaler.

For more information about the Cloud Controller, see [Cloud Controller](../../concepts/architecture/cloud-controller.html).

#### <a id='cc-debug-log-metrics'></a> Debug Log Metrics

Autoscaler logs the following metrics:

* `autoscale.cloud_controller.calls`: the number of requests that Autoscaler has made to the Cloud Controller
* `autoscale.cloud_controller.request_errors`: the number of requests that Autoscaler has made to the Cloud Controller that resulted in an error
* `autoscale.cloud_controller.scale_errors`: the number of requests to scale an app that have resulted in an error

<p class='note'><strong>Note:</strong> These metrics appear as log lines and are not currently emitted to Loggregator.</p>

#### <a id='cc-api-stopped'></a> Cloud Controller API is Stopped

When the Cloud Controller API is stopped, you see an autoscaling event similar to the following example:

<pre class='terminal'>Unable to scale due to Cloud Controller error.</pre>

This is a generic error that may indicate that an app has reached one or more of its resource quota limits. [Is this an alternate explanation for the error appearing, or does the Cloud Controller API stopping itself cause an app to reach a resource quota limit?]

You also see app logs similar to the following example:

<pre class='terminal'>
[APP/PROC/WEB/0] OUT level=error msg="[Cloud Controller - app summary] CloudController - route not found"`
</pre>

#### <a id='cc-unresponsive'></a> Cloud Controller is Unresponsive

When the Cloud Controller does not respond to requests from Autoscaler or Log Cache, you see an autoscaling event similar to the following example:

<pre class='terminal'>Unable to scale due to Cloud Controller error.</pre>

This is a generic error that may indicate that an app has reached one or more of its resource quota limits. [Is this an alternate explanation for the error appearing, or does CC being unresponsive itself cause an app to reach a resource quota limit?]

You also see app logs similar to the following example:

<pre class='terminal'>
[APP/PROC/WEB/0] OUT level=error msg="[Cloud Controller - app summary] failure connecting to cloud controller. Error: Get \"https://api.sys.example.com/v2/apps/85a6aff3-c739-4364-8df3-44f1aa50662b/summary\": context deadline exceeded (Client.Timeout exceeded while awaiting headers)"
[APP/PROC/WEB/1] OUT level=error msg="[Cloud Controller - app summary] CloudController - Failure"
</pre>

### <a id='mysql'></a> MySQL

Autoscaler stores persistent information about Autoscaler service bindings and the autoscaling rules you configure in a MySQL database.

If Autoscaler fails to select from or update the MySQL database, Autoscaler cannot scale your apps.

[Are there any debug log metrics for this?]

#### <a id='mysql-db-connection'></a> Autoscaler Fails to Connect to a MySQL Database

[Is there any example `cf autoscaling-events` output for this?]

You also see app logs similar to the following example:

<pre class='terminal'>
[APP/PROC/WEB/0] OUT level=error msg="Unable to find any enabled service bindings by instance id" error="driver: bad connection" instance_id=0
[APP/PROC/WEB/2] ERR [mysql] packets.go:37: unexpected EOF
</pre>

### <a id='diego-cells'></a> Diego Cells

Because Autoscaler is an app that you deploy through TAS for VMs, it uses the same resources that all other apps in your TAS for VMs deployment use. If all of
the Diego Cells that host [is this the right word?] Autoscaler fail, Autoscaler cannot scale your apps.

[Are there any debug log metrics for this?]

[Is there any example `cf autoscaling-events` output for this?]

[Are there any example logs for this?]


## <a id='scaling-Autoscaler'></a> Scaling Autoscaler

Autoscaler itself is an app that TAS for VMs deploys. By default, there are three instances of Autoscaler that scale the apps in your TAS for VMs deployment.
[But wait -- the <s>Highlander</s> troubleshooting topic says there can be only one!] Each instance of Autoscaler works independently of the other instances to try to claim ["Try to"? Is there something like a Diego Auctioneer-style process for this?] a set of apps that may need to be scaled and process them. [Does each instance claim a different set of apps each time?]

An Autoscaler instance considers [what does "consider" mean in this context?] apps in batches of 10 when making scaling decisions. The number of batches that each instance of Autoscaler can process is determined primarily by the number of metrics it retrieves from Log Cache for the apps within those batches. If Log Cache holds a large number of metrics for an app, Autoscaler does not begin processing the next batch until either it retrieves all of those metrics or the request times out. [So we're talking multiple batches of 10 apps being processed at the same time by one Autoscaler instance? Or is there a certain number of batches being processed sequentially within a certain timeframe?]

To ensure that Autoscaler scales all the apps for which you have configured autoscaling in a timely manner, you may need to increase the number of Autoscaler
instances [in a Diego Cell? A space/org?].

To increase the number of Autoscaler instances:

1. Navigate to the Ops Manager Installation Dashboard.

1. Click the **VMware Tanzu Application Service** tile.

1. Select **App Autoscaler**.

1. In the **Autoscaler instance count** field, enter the number of Autoscaler instances you want to deploy.

1. Click **Save**. [Do you need to re-deploy TAS for VMs at this point?]

1. Return to the Ops Manager Installation Dashboard.

1. Click **Review Pending Changes**.

1. Under the **VMware Tanzu Application Service** tile, click **Errands**. The **Errands** menu expands.

1. Ensure that the **App Autoscaler Errand** checkbox is activated.

1. Click **Apply Changes**.
