---
title: Using HTTP Latency as a Scaling Metric
owner: Autoscaler
---

This topic describes the Autoscaler HTTP latency metric and how to configure Autoscaler to use this metric in scaling rules.

## <a id="overview"></a> Overview of HTTP Latency

When a HTTP request is made to an app hosted on Tanzu Application Service, the [Gorouter](../../concepts/cf-routing-architecture.html) generates a number of metrics. One of these is a timer that records the time taken to handle the request, including the time required for the back-end app to respond.

For example, you might have a Service Level Agreement (SLA) specifying that 95% of requests for an app must be handled in less than 300 milliseconds. To help achieve this, you can add an autoscaling rule to scale out additional app instances when the latency reaches 250 milliseconds. This assumes that adding app instances lowers the HTTP latency before the business goal is impacted.

For caveats to scaling on HTTP latency, see [Caveats](#caveats).

## <a id="configure-autoscaler"></a> Configuring HTTP Latency as the Scaling Metric for an App

The procedures in this section describe how to configure Autoscaler to use HTTP latency as the scaling metric for an app. You can configure Autoscaler in the
following ways:

* Using the App Autoscaler Command-Line Interface (CLI). For more information, see [Configure Autoscaler Using the App Autoscaler CLI](#cli).

* Using Apps Manager. For more information, see [Configure Autoscaler Using Apps Manager](#apps-manager).

### <a id="cli"></a> Configure Autoscaler Using the App Autoscaler CLI

The App Autoscaler CLI is a Cloud Foundry Command-Line Interface (cf CLI) plug-in that adds terminal commands for controlling Autoscaler.

To configure Autoscaler to use HTTP latency as the scaling metric for an app using the App Autoscaler CLI:

1. Download the VMware Tanzu App Autoscaler CLI Plug-in from [VMware Tanzu Network](https://network.pivotal.io/products/pcf-app-autoscaler/).

1. Configure Autoscaler using one of the following methods.
    * Create a manifest file for Autoscaler. For more information, see [Create a Manifest File](#manifest).
    * Use terminal commands. For more information, see [Use Terminal Commands](#term-commands).

#### <a id="manifest"></a> Create a Manifest File

To declaratively specify Autoscaler configuration, you can create an Autoscaler manifest file.

**Note:** An Autoscaler manifest file is separate from an app manifest file, and is only used for Autoscaler configuration.</p>

To configure Autoscaler using a manifest file:

1. Create an Autoscaler instance in the same space as the app that you want Autoscaler to scale:

    ```sh
    cf create-service app-autoscaler PLAN-NAME SERVICE-NAME
    ```
    Where:
    * `PLAN-NAME` is the name of the service plan to use for the Autoscaler service instance.
    * `SERVICE-NAME` is the name of the service instance.

    For example:

    ```sh
    $ cf create-service app-autoscaler standard autoscaler
    ```

1. Bind the Autoscaler instance to the app:

    ```sh
    cf bind-service APP-NAME autoscaler
    ```
    Where `APP-NAME` is the name of the app you want Autoscaler to scale.

    For example:

    ```sh
    $ cf bind-service example-app autoscaler
    ```

1. Create the Autoscaler manifest file:

    ```yaml
    ---
    instance_limits:
      min: 10
      max: 100
    rules:
    - rule_type: http_latency
      rule_sub_type: avg_95th
      threshold:
        min: 125
        max: 250
    scheduled_limit_changes: []
    ```

    This manifest defines a `http_latency` rule and configures three settings:

    1. The minimum threshold in milliseconds (`threshold.min`). If the average request latency drops below this number, Autoscaler scales the app down. In this example, the minimum threshold is set to 125 milliseconds.

    1. The maximum threshold in milliseconds (`threshold.max`). If the average request latency rises above this number, Autoscaler scales the app up. In this example, the maximum threshold is set to 250 milliseconds. In general, VMware recommends that you set the value for maximum threshold to at least twice the value of the minimum threshold, to avoid excessive cycling.

    1. The percentile (`rule_sub_type`), either `avg_95th` or `avg_99th`. This is the percentile that Autoscaler uses in scaling decisions. In this example, the percentile is set to `avg_95th`, so Autoscaler ignores requests that fall outside of the 95th percentile and averages the remaining 95% of requests.

    The manifest also specifies a minimum instance limit of 10 and a maximum instance limit of 100.

1. Apply the Autoscaler manifest for your app:

    ```sh
    cf configure-autoscaling APP-NAME MANIFEST-FILENAME
    ```
    Where:
    * `APP-NAME` is the name of the app that you want Autoscaler to scale.
    * `MANIFEST-FILENAME` is the filename of the Autoscaler manifest.

    For example:

    ```sh
    $ cf configure-autoscaling example-app autoscaler-manifest.yaml
    ```

VMware recommends that you perform load testing of your app to verify that your configured rules are correct. For more information, see [Using App Autoscaler in Production](productionizing-autoscaler.html).

When scaling because of an increase in latency, Autoscaler records the observed latency in the scaling event. For example:

```sh
$ cf autoscaling-events example-app

Time                   Description
2022-05-23T21:47:45Z   Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
```

#### <a id="term-commands"></a> Use Terminal Commands

You can configure Autoscaler to use the HTTP latency metric using cf CLI commands.

To configure using the cf CLI:

1. Create an Autoscaler instance in the same space as the app that you want Autoscaler to scale:

    ```sh
    cf create-service app-autoscaler PLAN-NAME SERVICE-NAME
    ```
    Where:
    * `PLAN-NAME` is the name of the service plan to use for the Autoscaler service instance.
    * `SERVICE-NAME` is the name of the service instance.

    For example:

    ```sh
    $ cf create-service app-autoscaler standard autoscaler
    ```

1. Bind the Autoscaler instance to the app:

    ```sh
    cf bind-service APP-NAME autoscaler
    ```
    Where `APP-NAME` is the name of the app that you want Autoscaler to scale.

    For example:

    ```sh
    $ cf bind-service example-app autoscaler
    ```

1. Update autoscaling limits for the app that you want to scale:

    ```sh
    cf update-autoscaling-limits APP-NAME MIN-INSTANCE-LIMIT MAX-INSTANCE-LIMIT
    ```
    Where:
    * `APP-NAME` is the name of the app that you want Autoscaler to scale.
    * `MIN-INSTANCE-LIMIT` is the minimum instance limit.
    * `MAX-INSTANCE-LIMIT` is the maximum instance limit.

    For example:

    ```sh
    $ cf update-autoscaling-limits example-app 10 100
    ```

1. Enable autoscaling for the app:

    ```sh
    cf enable-autoscaling APP-NAME
    ```
    Where `APP-NAME` is the name of the app that you want Autoscaler to scale.

    For example:

    ```sh
    $ cf enable-autoscaling example-app
    ```

1. Add a `http_latency` rule:

    ```sh
    cf create-autoscaling-rule APP-NAME http_latency MIN MAX --subtype SUBTYPE
    ```
    Where:
    * `APP-NAME` is the name of the app that you want Autoscaler to scale.
    * `MIN` is the minimum threshold in milliseconds. If the average request latency drops below this number, Autoscaler scales the app down. 
    * `MAX` is the maximum threshold in milliseconds. If the average request latency rises above this number, Autoscaler scales your app up. In general, VMware recommends that you set the value for maximum threshold to at least twice the value of the minimum threshold, to avoid excessive cycling.
    * `SUBTYPE` is the percentile on which Autoscaler bases scaling decisions, either `avg_95th` or `avg_99th`. For example, `avg_95th` ignores requests that fall outside of the 95th percentile and averages the remaining 95% of requests.

    For example:

    ```sh
    $ cf create-autoscaling-rule example-app http_latency 125 250 --subtype avg_95th
    ```

VMware recommends that you perform load testing of your app to verify that your configured rules are correct. For more information, see [Using App Autoscaler in Production](productionizing-autoscaler.html).

When scaling because of an increase in latency, Autoscaler records the observed latency in the scaling event. For example:

```sh
$ cf autoscaling-events example-app

Time                   Description
2022-05-23T21:47:45Z   Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
```

### <a id="apps-manager"></a> Configure Autoscaler Using Apps Manager

To configure Autoscaler to use HTTP latency as the scaling metric for an app using Apps Manager:

1. In Apps Manager, navigate to the **Manage Autoscaling** -> **Edit Scaling Rules** dialog box.

1. Select **Add Rule**.

1. Configure the fields as follows:

    | Field                                        | Instructions                             |
    |----------------------------------------------|------------------------------------------|
    | <strong>Rule Type</strong>                   | Select <strong>HTTP Latency</strong>     |
    | <strong>Scale down if less than</strong>     | Enter the minimum threshold in milliseconds. If average request latency falls below this number, Autoscaler scales the app down. |
    | <strong>Scale up if more than</strong>       | Enter the maximum threshold in milliseconds. If average request latency rises above this number, Autoscaler scales the app up. In general, VMware recommends that you set the value for maximum threshold to at least twice the value of the minimum threshold, to avoid excessive cycling. |
    | <strong>Percent of traffic to apply</strong> | Select either <strong>95%</strong> or <strong>99%</strong>. Autoscale uses this percentile to make autoscaling decisions (for example, if you select <strong>95%</strong>, Autoscaler ignores requests outside of the 95th percentile and averages the remaining 95% of requests). |

1. Click **Save**.

VMware recommends that you perform load testing of your app to verify that your configured rules are correct. For more information, see [Using App Autoscaler in Production](productionizing-autoscaler.html).

When scaling because of an increase in latency, Autoscaler records the observed latency in the scaling event. In Apps Manager, you can view the scaling event in the Event History, under **Manage Autoscaling**. For example:

> Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.

For more information, see [Configure Autoscaling for an App](using-autoscaler.html#config).

## <a id="caveats"></a> Caveats

The following cases require special consideration when using HTTP latency as a metric for autoscaling.

### <a id="multiple-endpoints"></a> Multiple Endpoints with Different Latency Characteristics

In an app that exposes multiple endpoints, one endpoint might be slower than the others. In this case, HTTP latency might not be an ideal autoscaling metric, because the calculated latency is an average of the latency across all app endpoints. For example, a large number of requests to a fast endpoint drags down the average HTTP latency, and can prevent Autoscaler from scaling up the app.

### <a id="downstream-services"></a> Downstream Services

Sometimes HTTP request latency results from downstream dependencies (for example, other microservices). If the app has a slow downstream dependency, this can also increase the latency of the app. If this is the case, the downstream dependency must be scaled up or improved, and autoscaling your app does not improve performance.

Latency caused by other external factors, such as network congestion or database performance, can also cause problems when using the HTTP latency metric for Autoscaler. Scaling up by adding app instances does not decrease latency resulting from these factors.

**Important:** If performance issues are caused by a downstream service, scaling up your app might _increase_ HTTP latency as the additional app instances add load on the downstream service.

### <a id="caveat-c2c-networking"></a> Container to Container (C2C) Networking

Autoscaling based on HTTP latency is currently only supported for apps that receive requests directly from the Gorouter.

If your system includes back end HTTP services that are accessed directly from another app using Container to Container (C2C) networking, the Gorouter does not generate HTTP events for these requests, and Autoscaler cannot scale based on the requests. In this case, configure Autoscaler to scale based on an alternative built-in metric or a custom metric.

### <a id="log-cache-eviction"></a> High-Traffic Apps

Autoscaler retrieves HTTP metrics from Log Cache, which has a default limit of 100,000 envelopes per app. If your app receives a large number of HTTP requests, or has very verbose logging, Autoscaler might only have access to a subset of the generated timer envelopes for the period. This causes a bias in the calculated HTTP latency. In most cases, the calculated latency still approximates the request latency.

For more information, see [Using App Autoscaler in Production](productionizing-autoscaler.html).

### <a id="infrequent-access"></a> Infrequently Accessed Apps

If your app is infrequently accessed and has a high-latency response, Autoscaler might continue to scale up the app because no other HTTP latency metrics are available to restore the average. In this case, Autoscaler usually finishes scaling up after the original request falls outside of the metric collection interval.

For more information about the metric collection interval, see [About App Autoscaler](about-app-autoscaler.html#about-scaling-decisions).
