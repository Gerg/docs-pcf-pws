---
title: Using HTTP Latency as a Scaling Metric
owner: Autoscaler
---

This topic describes


## <a id="overview"></a> Overview of HTTP Latency

When a HTTP request is made to an app hosted on Tanzu Application Service the [gorouter](../../concepts/cf-routing-architecture.html) generates a number of metrics. One of these is a timer recording the time taken to handle the request including the time taken by the back end app to respond.

Let's say that you have a Service Level Agreement (SLA) that 95% of your app requests for a given app should be handled in less than 300ms.

To help achieve this you can add an autoscaling rule to scale out additional app instances when the latency hits 250ms. This assumes that adding app instances will lower the HTTP latency before your business goal is impacted.

There are a number of caveats to scaling on HTTP latency that are documented in the [Caveats section](#caveats).


## <a id="configure-autoscaler"></a> Configuring HTTP Latency as the Scaling Metric for an App

The procedures in this section describe how to configure Autoscaler to use HTTP latency as the scaling metric for an app. You can configure Autoscaler in the
following ways:

* Through the App Autoscaler Command-Line Interface (CLI). For more information, see [Configure Autoscaler Through the App Autoscaler CLI](#cli) below.

* Through Apps Manager. For more information, see [Configure Autoscaler Through Apps Manager](#apps-manager) below.

### <a id="cli"></a> Configure Autoscaler Through the App Autoscaler CLI

The App Autoscaler CLI is an extension of the Cloud Foundry Command-Line Interface (cf CLI) that allows you to control Autoscaler using terminal commands.

To configure Autoscaler to use HTTP latency as the scaling metric for an app using the App Autoscaler CLI:

1. Download the VMware Tanzu App Autoscaler CLI Plugin from [VMware Tanzu Network](https://network.pivotal.io/products/pcf-app-autoscaler/).

1. Configure Autoscaler using one of the following methods:
    * Create a manifest file for Autoscaler. For more information, see [Create a Manifest File](#manifest) below.
    * Use terminal commands. For more information, see [Use Terminal Commands](#term-commands) below.

#### <a id="manifest"></a> Create a Manifest File

To declaratively specify the autoscaler configuration you can create an autoscaler manifest file. This is a separate file to any existing app manifest file, just for autoscaler. [Why would you want to do this, and not just use commands?]

To configure Autoscaler using a manifest file:

1. If you have not already created an instance of Autoscaler in the space where your app is deployed, run:

    ```
    cf create-service app-autoscaler standard autoscaler
    ```

1. Bind Autoscaler to the app you want to scale by running:

    ```
    bind-service APP-NAME autoscaler
    ```
    Where `APP-NAME` is the name of the app you want Autoscaler to scale.


Now create the autoscaler manifest file:

```yaml
---
instance_limits:
  min: 10
  max: 100
rules:
- rule_type: http_latency
  rule_sub_type: avg_95th
  threshold:
    min: 125
    max: 250
scheduled_limit_changes: []
```

The above manifest defines a `http_latency` rule made up of three settings:

1. The minimum threshold in milliseconds (`min` in the manifest). If the average request latency drops below this number then autoscaler will scale your app down. In the example the minimum threshold is set to 125 milliseconds.

1. The maximum threshold in milliseconds (`max` in the manifest). If the average request latency goes above this number then autoscaler will scale your app up. In the example the maximum threshold is set to 250 milliseconds.

1. The percentile (`rule_sub_type` in the manifest). Either `avg_95th` or `avg_99th`. This is the percentile to base the autoscaling decision on. For example `avg_95th` will ignore requests that fall outside of the 95th percentile and average the remaining 95% of requests.

In general, the value for maximum threshold should be at least twice the value of the minimum threshold to avoid excessive cycling.

The manifest also specifies a minimum instance limit of 10 and a maximum instance limit of 100.

You can then apply the autoscaler manifest for the example-app with the following command:

```
$ cf configure-autoscaling example-app autoscaler-manifest.yml
```

We recommend that you perform load testing of your app in order to validate that your configured rules are correct. Refer to the [Using App Autoscaler in Production](productionizing-autoscaler.html) topic for more information.

When scaling Autoscaler will record in the scaling event the latency it observed that caused the scaling decision.

For example:

```
$ cf autoscaling-events example-app

Time                   Description
2022-05-23T21:47:45Z   Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
```

#### <a id="term-commands"></a> Use Terminal Commands

First, create an instance of the autoscaler service and bind it to the app you want to scale:

```
$ cf create-service app-autoscaler standard autoscaler
$ cf bind-service example-app autoscaler
```

You can skip the `cf create-service` command if you already have an instance of the autoscaler in that space.

You can then update the autoscaling limits and enable autoscaling for the app. Here we have specified a minimum instance limit of 10 and a maximum instance limit of 100.

```
$ cf update-autoscaling-limits example-app 10 100
$ cf enable-autoscaling example-app
```

Now you can add a `http_latency` rule:

```
$ cf create-autoscaling-rule example-app http_latency 125 250 --subtype avg_95th
```

Apart from the app name you need to specify three settings:

1. The minimum threshold in milliseconds. If the average request latency drops below this number then autoscaler will scale your app down. In the example the minimum threshold is set to 125 milliseconds.

1. The maximum threshold in milliseconds. If the average request latency goes above this number then autoscaler will scale your app up. In the example the maximum threshold is set to 250 milliseconds.

1. The percentile (either `avg_95th` or `avg_99th`). This is the percentile to base the autoscaling decision on. For example `avg_95th` will ignore requests that fall outside of the 95th percentile and average the remaining 95% of requests.

In general, the value for maximum threshold should be at least twice the value of the minimum threshold to avoid excessive cycling.

We recommend that you perform load testing of your app in order to validate that your configured rules are correct. Refer to the [Using App Autoscaler in Production](productionizing-autoscaler.html) topic for more information.

When scaling Autoscaler will record in the scaling event the latency it observed that caused the scaling decision.

For example:

```
$ cf autoscaling-events example-app

Time                   Description
2022-05-23T21:47:45Z   Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.
```

### <a id="apps-manager"></a> Configure Autoscaler Through Apps Manager

1. Within Apps Manager navigate to the **Manage Autoscaling** -> **Edit Scaling Rules** dialog and select **Add Rule**.

1. Select **Rule Type** HTTP Latency.

1. For **Scale down if less than** enter the minimum threshold in milliseconds. If the average request latency drops below this number then autoscaler will scale your app down.

1. For **Scale up if more than** enter the maximum threshold in milliseconds. If the average request latency goes above this number then autoscaler will scale your app up.

1. For **Percent of traffic to apply** select either 95% or 99%. This is the percentile to base the autoscaling decision on. For example choosing 95% will ignore requests that fall outside of the 95th percentile and average the remaining 95% of requests.

1. Click **Save**.

In general, the value for maximum threshold should be at least twice the value of the minimum threshold to avoid excessive cycling.

We recommend that you perform load testing of your app in order to validate that your configured rules are correct. Refer to the [Using App Autoscaler in Production](productionizing-autoscaler.html) topic for more information.

When scaling Autoscaler will record in the scaling event the latency it observed that caused the scaling decision. This is visible in Apps Manager in the Event History under **Manage Autoscaling**.

> Scaled up from 10 to 11 instances. Current HTTP Latency of 1010.96ms is above upper threshold of 250.00ms.

For more information refer to [Configure Autoscaling for an App](using-autoscaler.html#config).


## <a id="caveats"></a> Caveats

HTTP latency is a useful metric to scale on, but there are a number of caveats around its use that you should consider.

### <a id="caveat-multiple-endpoints"></a> Multiple endpoints with different latency characteristics

You might have an app that exposes multiple endpoints, one of which is particularly slow. In this instance HTTP latency may not be a good fit because the calculated latency will be an average of the latency across all the app endpoints.

For example a large number of requests to a fast endpoint on the app will drag down the average HTTP latency and may prevent the app from scaling up.

### <a id="caveat-downstream-services"></a> Downstream services

If your app makes use of downstream services then scaling out the number of app instances may have no effect on the latency without addressing other scaling bottlenecks.

Latency can be more a factor of downstream dependencies (for example other microservices) than it is of the current app. If you have a slow downstream dependency this can increase the latency of your app as well. Scaling out your app will not help with performance as the downstream dependency is what really needs to be scaled up or improved.

Latency added by other external factors like network congestion or database performance can also cause issues with HTTP latency based scaling. Increased latency from these factors will not be improved by scaling out app instances.

Note that adding additional app instances through scaling when the constraint is not the app could result in HTTP latency *increasing* as the app instances may increase the load on the external service.

### <a id="caveat-c2c-networking"></a> Container to Container (C2C) Networking

Scaling on HTTP latency is currently only supported for apps that receive requests directly via the gorouter.

If your system includes backend HTTP services that are accessed directly from another app via Container to Container (C2C) networking then there will not be HTTP events generated by gorouter for those requests and autoscaler will be unable to scale on them. Currently for those cases you will need to use an alternative built-in metric or expose a custom metric.

### <a id="caveat-log-cache-eviction"></a> High-traffic apps

Autoscaler retrieves the HTTP metrics from Log Cache which has a default limit of 100,000 envelopes per app.

If you are attempting to scale an app that receives a large number of HTTP requests (or has very verbose logging) it is possible for Autoscaler to only have access to a subset of the generated timer envelopes for the period, biasing the calculated HTTP latency.

In most cases the calculated latency should still approximate the request latency but this is a source of bias you should be aware of.

Refer to the [Using App Autoscaler in Production](productionizing-autoscaler.html) topic for more information.

### <a id="caveat-infrequent-access"></a> Infrequently accessed apps

If your app is infrequently accessed and the latency for that response is high then it is possible for autoscaler to continue to scale up the app as there are no other HTTP latency metrics available that would restore the average.

In this case the scaling up should complete once the original request has fallen outside of the metric collection interval. Refer to the [About App Autoscaler](about-app-autoscaler.html#about-scaling-decisions) topic for more information about the metric collection interval.
