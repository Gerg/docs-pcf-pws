---
title: Using App Autoscaler in Production
owner: App Autoscaler
---

This topic describes how to use App Autoscaler for scaling your apps in a production environment.


## <a id='overview'></a> Overview of

[Insert something about scaling apps according to needs of the deployment, scaling goals, etcetera here.]


## <a id='autoscaling-limits'></a> Autoscaling Limits

The autoscaling limits for an app are the upper and lower limits for the number of instances Autoscaler can create for that app. The lower scaling limit is
the minimum number of instances to which Autoscaler can scale an app down, and the upper scaling limit is the maximum number of instances to which Autoscaler
can scale an app up. Setting appropriate upper and lower scaling limits ensures that Autoscaler does not create too few or too many app instances.

Apps often have dependencies on an external app or service that can constrain Autoscaler's ability to scale app instances effectively. Because of this,
scaling instances up past a certain number may not address problems you encounter until you address issues within those dependencies. For example, if all of
your app instances use a single shared MySQL service instance, scaling the number of instances up might cause them to reach the maximum number of connections
allowed by MySQL. You must address this constraint before you can scale your app further.

VMware recommends load-testing your app to confirm that you have set appropriate scaling limits before pushing the app to a production environment. For more
information, see [Load-Testing Your App](#load-testing) below.

### <a id='configure-autoscaling-limits'></a> Configure Autoscaling Limits

To configure the upper and lower scaling limits for an app:

1. In a terminal window, run:

    ```
    cf update-autoscaling-limits APP-NAME LOWER-SCALING-LIMIT UPPER-SCALING-LIMIT
    ```
    Where:
    * `APP-NAME` is the name of the app for which you want to configure autoscaling limits.
    * `LOWER-SCALING-LIMIT` is the minimum number of instances you want Autoscaler to create for the app.
    * `UPPER-SCALING-LIMIT` is the maximum number of instances you want Autoscaler to create for the app.
    <br>
    For example, running the following command sets the lower scaling limit for `example-app` to `20` and the upper scaling limit to `100`:

    ```
    cf update-autoscaling-limits example-app 20 100
    ```


## <a id='quotas'></a> Resource Quotas

Each TAS for VMs deployment has a quota plan that defines memory, service, and instance usage quotas for all apps and services within the deployment. For
example, one quota plan might allow up to 10 services, 10 routes, and 2 GB of RAM, while another might offer 100 services, 100 routes, and 10 GB of RAM.

When setting autoscaling limits for Autoscaler, you must ensure that the resource quotas allocated to the app are sufficient to allow Autoscaler to scale the
app within those limits. If the resource quotas that are available for the app to use are lower than the resources required to meet the upper scaling limit
you set, Autoscaler cannot scale up to that upper scaling limit.

You can create quota plans for spaces and orgs. If you do not have permissions to create quota plans or you need an existing quota limit increased, contact
the operator for your Ops Manager deployment.

When defining quotas for a space or org, you must consider not only the scaling needs of an individual app, but the potential scaling needs of other apps in
the same space or org.

For more information about creating and modifying quota plans for TAS for VMs deployments, see [Creating and Modifying Quota
Plans](../../adminguide/quota-plans.html).

### <a id='quotas-limiting-scaling'></a> Reviewing Space and Org Quotas

To verify that Autoscaler is unable to scale your app due to hitting the quota limit, review the resources your app uses and the quota for the space in which the app is deployed.

For example, you might have your app configured with a memory limit of 1G per app instance. The effective memory use is `1G * number of app instances`. You can then discover which org and space quotas are assigned (with `cf org` and `cf space`) and view the total memory and instance memory available under that quota with `cf org-quota` or `cf space-quota`.

### <a id='per-app-limits'></a> Setting Per-App Limits

VMware recommends considering the individual resource limits for each app and whether they are appropriate. Are you pushing an app with significantly more RAM or disk than it will use? Reducing the per-instance requirements for an app should allow you to scale out to more app instances with the same underlying resources.

There is a caveat here though: for TAS for VMs, your app instance CPU entitlement is linked to memory allocated, so reducing the memory available to an app instance also reduces its CPU shares.


## <a id='choosing-a-metric'></a> Choosing a Scaling Metric

For each app, consider which metric is most appropriate to use to scale it. Autoscaler supports defining rules for a variety of different types of built-in metrics, and can extend them by using custom metrics.

Initially, VMware recommends that you choose a single built-in metric such as **HTTP Latency** or **RabbitMQ Queue Depth**. Avoid defining multiple autoscaling rules for the same app.

**HTTP Latency:** Use this rule type if your app receives HTTP requests directly from the Gorouter, and you want to scale out the number of app instances as the average latency of the requests increases. Before choosing this metric as your scaling metric, consider whether scaling your app might increase latency. For example, if your additional app instances are contending for the same external resource.

**RabbitMQ Queue Depth:** Use this rule type if your app consumes work from a RabbitMQ queue, and you want to scale out the number of app instances to consume work from the queue more quickly.

If your app uses multiple TAS for VMs apps [as in services?], consider whether you should use the same scaling metric across those apps. Using the same scaling metric may allow you to better understand the scaling behavior of your deployment.


## <a id='c2c-networking'></a> Autoscaler and C2C Networking

When defining your autoscaling rules, remember that HTTP Latency and HTTP Throughput are currently only supported for apps that receive requests directly through the Gorouter.

If your system includes back end HTTP services that are accessed directly from another app through Container to Container (C2C) networking, Autoscaler is unable to scale them. This is because there are no HTTP events generated by Gorouter for those requests. In these cases, you must use an alternative built-in metric, or expose a custom metric.


## <a id='scaling-factors'></a> Scaling Factors

Autoscaler supports defining scale up and scale down factors to control how quickly it scales your app in each direction. These are defined on the app itself rather than on specific autoscaling rules.

Setting a larger scale up factor means that your app scales up more responsively to a metric. For each metric you are scaling, consider what circumstances scaling on that metric is unhelpful. For example, is HTTP Latency occurring due to an unresponsive downstream service or networking issues? Could configuring Autoscaler to aggressively scale up app instances make the issue worse?

It is common to define a scale down factor that is lower than the scale up factor. This allows your app to scale up faster, and then to return to an earlier baseline number of instances more slowly by stepping down in smaller increments.

The following command sets the scaling factors for `example-app` that scale up by `20` instances at a time, but scale down by `10` instances at a time:

<pre class='terminal'>cf update-autoscaling-factors example-app 20 10</pre>





## <a id='load-testing'></a> Load-Testing Your App

To ensure that you have configured effective autoscaling for your app, VMware recommends that you load-test the app. This demonstrates that your app scales in most cases when the metric you are scaling changes.

It is important to load-test in an environment that is as similar to your production deployment as possible.

### <a id='generating-load'></a> Generating Load

Organizations use a variety of tools for load-testing. The method for generating load varies, but is tied to the metric that you have selected to scale on.

Ideally, you want to ramp up the number of requests or messages in the queue step by step. You can confirm at each step that Autoscaler is scaling as expected.

The `cf autoscaling-events` command allows you to monitor scaling events as the app is scaled.

### <a id='autoscaler-calculating-correctly'></a> Confirming that Autoscaler Sees the Correct Metric Value

For some rule types such as HTTP Throughput, you can compare the load from the load-testing tool against the calculated value reported by Autoscaler to confirm that Autoscaler is reporting similar numbers. If these diverge, such as if Autoscaler reports a lower number of requests per second than the load test, this may indicate that the underlying platform must be scaled further.

### <a id='confirm-scaling-out-is-effective'></a> Confirming that Adding more app Instances has an Effect

For each scaling step that you test, you should confirm that:

* Autoscaler notices that a metric has crossed the threshold

* Autoscaler has successfully scaled your app instances

* Subsequent scaling operations adding more app instances bring the metric back to below the threshold

If Autoscaler continues to scale your app instances without effect, that suggests that there is a bottleneck elsewhere in the system. You might be able to identify the issue with app monitoring, or you might need to work with your operations team to identify the issue.
